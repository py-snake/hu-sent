\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[magyar]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{tabularx}
\usepackage{booktabs}

\title{Magyar nyelvű Szentimentanalízis Projekt}
\author{Név}
\date{\today}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\small\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\lstdefinelanguage{json}{
    basicstyle=\ttfamily\small,
    showstringspaces=false,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    literate=
     *{á}{{\'a}}1
      {é}{{\'e}}1
      {í}{{\'i}}1
      {ó}{{\'o}}1
      {ö}{{\"o}}1
      {ő}{{\H{o}}}1
      {ú}{{\'u}}1
      {ü}{{\"u}}1
      {ű}{{\H{u}}}1
      {Á}{{\'A}}1
      {É}{{\'E}}1
      {Í}{{\'I}}1
      {Ó}{{\'O}}1
      {Ö}{{\"O}}1
      {Ő}{{\H{O}}}1
      {Ú}{{\'U}}1
      {Ü}{{\"U}}1
      {Ű}{{\H{U}}}1
}

\lstdefinestyle{docker}{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    rulecolor=\color{gray},
    backgroundcolor=\color{gray!5},
    tabsize=2,
    showstringspaces=false
}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Projekt Áttekintése}

A projekt célja egy magyar nyelvű szentimentanalízis modell fejlesztése Pythonban, amely a HuSST adatkészletet használja. A modellel szembeni elvárás, hogy képes legyen a szövegeket negatív, semleges és pozitív kategóriákba sorolni.

\section{Módszertan}

A cél megvalósításához a huBERT \textit{(Hungarian Universal Bidirectional Encoder Representations from Transformers)} betanított neurális hálót fogom felhasználni alapmodellként. Az előre betanított neurális háló nagyon jó kiindulási alapként szolgál, mivel magyar nyelvű adatokon tanították, tehát általános magyar nyelvtudással rendelkezik. Képes a szövegek értelmezésére és feldolgozására, viszont általánosságban elmondható, hogy ezeket az alapmodelleket további tanítással kell kiegészíteni ha specifikusan egy bizonyos célra szeretnénk használni az alap tudását.

Jelen feladatban a HuSST adathalmazzal fogok további tanítást végezni a modellen. A HuSST, mint korábban említésre került, magyar nyelvű kijelentéseket tartalmaz és az azokhoz tartozó címkéket. A címke lehet negatív, semleges vagy pozitív. Ezek alapján kerül besorolásra az adott szöveg. 

\section{Dataset}

A bevezetőben ismertetett két forrást fogom használni a projekt megvalósításához.

\begin{itemize}
    \item huBERT base model (Hungarian Universal Bidirectional Encoder Representations from Transformers)
    \item HuSST dataset (Hungarian Stanford Sentiment Treebank)
\end{itemize}

\subsection{huBERT bemutatása}

A huBERT egy magyar nyelvű, transzformátor alapú nyelvi modell, amelyet a SZTAKI fejlesztett ki. A modell a BERT architektúrát követi, és kifejezetten a magyar nyelv sajátosságainak kezelésére optimalizálták. A tanítást az úgynevezett \textit{Common Crawl} adatbázis magyar nyelvű részén végezték szűrések és deduplikációk után, valamint a magyar Wikipédián alapulva. A modell 111 millió paraméterrel rendelkezik.

\subsection{A huBERT alkalmazási lehetőségei}

A huBERT modellt különféle magyar nyelvű NLP \textit{(Natural Language Processing)} feladatokhoz használhatjuk:

\begin{itemize}
    \item Szövegosztályozás
    \item Névfelismerés (NER \textit{(Named Entity Recognition)})
    \item Szövegrészletezés (Chunking)
    \item Kérdésmegválaszolás
    \item Szöveggenerálás
\end{itemize}

\section{Implementáció, technológiák}

A projekt megvalósítása során Python nyelven dolgozom a gépi tanulás és a webes felület implementációjához. A modell fejlesztéséhez a PyTorch keretrendszert, az adatkezeléshez és előfeldolgozáshoz a pandas és numpy könyvtárakat, míg a tokenizáláshoz a Hugging Face transformers könyvtárat használom. A megoldás konténerizálását Docker segítségével oldom meg. A webes felület Flask webszerverrel készül, míg az adatok tárolása PostgreSQL adatbázisban történik.

A modell és a ráépülő webes rendszer Pythonban készül a következő könyvtárak felhasználásával:

\subsection{Alapvető Python könyvtárak}

\begin{itemize}
    \item \texttt{os}: Operációs rendszer szintű műveletek (fájlkezelés, környezeti változók)
    \item \texttt{json}: JSON adatok szerializálása és deszerializálása
    \item \texttt{re}: Reguláris kifejezések a szövegfeldolgozáshoz (regex)
    \item \texttt{time}: Időzítési műveletek és késleltetések
    \item \texttt{logging}: Alkalmazás naplózásának konfigurálása
    \item \texttt{zlib}: Adattömörítés és kicsomagolás
\end{itemize}

\subsection{Adatgyűjtés és Feldolgozás}

\begin{itemize}
    \item \texttt{requests}: HTTP kérések küldése és fogadása
    \item \texttt{BeautifulSoup}: HTML és XML dokumentumok elemzése
    \item \texttt{concurrent.futures}: Párhuzamos feldolgozás megvalósítása
    \item \texttt{urllib.parse}: URL címek kezelése
\end{itemize}

\subsection{Adatbázis Kapcsolatok}

\begin{itemize}
    \item \texttt{psycopg2}: PostgreSQL adatbázis-kezelőhöz való csatlakozás
    \item \texttt{SQLAlchemy}: ORM \textit{(Object-Relational Mapping)} rendszer
    \item \texttt{datetime}: Dátum és időkezelés
\end{itemize}

\subsection{Webes Felület}

\begin{itemize}
    \item \texttt{Flask}: Mikrokeretrendszer webalkalmazás fejlesztéséhez
    \item \texttt{flask\_login}: Felhasználói munkamenetek kezelése
    \item \texttt{werkzeug.security}: Jelszavak biztonságos tárolása és ellenőrzése
\end{itemize}

\subsection{Machine Learning és NLP}

\subsubsection{PyTorch Könyvtárak}
\begin{itemize}
    \item \texttt{torch}: Tenzorműveletek és GPU támogatás
    \item \texttt{torch.nn}: Neurális hálók építéséhez szükséges modulok
    \item \texttt{torch.optim}: Optimalizálási algoritmusok (Adam, SGD)
    \item \texttt{torch.utils.data}: Adatbetöltés és előfeldolgozás
\end{itemize}

\subsubsection{NLP-specifikus Könyvtárak}
\begin{itemize}
    \item \texttt{transformers}: Előtanított nyelvi modellek kezelése
    \item \texttt{datasets}: Nagy nyelvi adathalmazok betöltése és kezelése
    \item \texttt{sklearn.metrics}: Osztályozási metrikák számítása
\end{itemize}

\subsection{Adatelemzés}
\begin{itemize}
    \item \texttt{jupyter}: Interaktív notebook környezet
    \item \texttt{jupyterlab}: Fejlettebb notebook felület
    \item \texttt{pandas}: Adatok táblázatos kezelése és elemzése
    \item \texttt{numpy}: Numerikus számítások és tömbműveletek
    \item \texttt{tqdm}: Folyamatjelző sáv iterációkhoz
\end{itemize}

\subsection{Konténerizáció}
\begin{itemize}
    \item \texttt{Docker}: Alkalmazás konténerbe csomagolása
    \item \texttt{Docker Compose}: Többkonténeres alkalmazások kezelése
\end{itemize}

\subsection{Függőségek}

A projekt függőségeit a \texttt{requirements.txt} fájl tartalmazza.

\section{Alapvető szentimentanalízis modell elkészítése}

A szentimentanalízis modell elkészítése több fő lépésből áll, ezek bemutatása fog következni.

\subsection{1. lépés: Az előre tanított BERT modell betöltése és a tanítás felparaméterezése}

Első lépésként a kiválasztott nyelvhez illeszkedő előre tanított neurális háló betöltésére van szükség. Jelen esetben a magyar nyelvfeldolgozáshoz a \texttt{SZTAKI-HLT/hubert-base-cc} modellre esett a választás.
A konkrét megvalósítás szemléltetése érdekében beillesztem az alábbi kódrészletet, ahol a felparaméterezés látható.

\begin{lstlisting}[language=Python,caption=Modell konfiguráció]
# Configuration - Using a publicly available Hungarian model
MODEL_NAME = "SZTAKI-HLT/hubert-base-cc"  # Public Hungarian BERT model
BATCH_SIZE = 16
MAX_LENGTH = 128
EPOCHS = 3
LEARNING_RATE = 2e-5
NUM_CLASSES = 3  # negative, neutral, positive
LABEL_MAP = {"negative": 0, "neutral": 1, "positive": 2} # Create label mapping
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
\end{lstlisting}

\subsubsection{Batch size}

A kódrészletben a \textit{Batch size} paraméter határozza meg, hogy egy \textit{Epoch-ban} hány minta legyen felhasználva a tanításhoz. A jelenlegi 16-os \textit{batch size} azt jelenti, hogy ekkora csomagokban fog zajlani a tanítás. Ez az érték kiegyensúlyozott a memóriahasználat és a tanítási sebesség között.

\subsubsection{Max length}

A \textit{Max length} paraméter 128-as értéke azt állítja be, hogy legfeljebb 128 tokent tartalmazhasson egy minta. Pontosabban kifejezve, egy adott bemenet hossza legfeljebb 128 tokenből állhat, ahol egy token például egy szónak, szórészletnek, vagy írásjelnek feleltethető meg. A tokenek konkrét hossza és a tokenizáció menete a eltérő lehet különböző nyelvek között. Jelenleg a rövidebb szövegeket \textit{padding} egészíti ki, a hosszabbak csonkolásra kerülnek.

\subsubsection{Num classes és Label map}

A \textit{Num classes} és a \textit{Label map} a tanítás során használni kívánt kategóriákat határozza meg. Esetünkben három kategória létezik: a negatív, semleges, és pozitív. A szöveges címkéket numerikus értékekre képezi le, ami szükséges a neurális háló számára.

\subsubsection{Epochs}

Az \textit{Epochs} a tanítási iterációk számát határozza meg. Egy epoch azt jelenti, hogy a teljes tanító adathalmazon egyszer végighaladt a modell. Túl sok epoch túltanításhoz \textit{(overfitting)} vezethet, míg kevesebb epoch alultanítást \textit{(underfitting)} eredményez. A jelenleg megadott érték elegendő lehet egy megfelelő tanításhoz, erőforrások hiányában nem növelem, mert azzal jelentősen növekedne a tanításhoz szükséges idő is.

\subsubsection{Learning rate}

A \textit{Learning rate} a tanulási ráta, ami meghatározza, hogy mennyit változzon a modell súlya egy lépésben. Túl magas érték instabil tanításhoz vezet, míg túl alacsony érték lassú konvergálást eredményez. Jelenleg egy általánosan elfogadott érték került beállításra.

\subsection{2. lépés: Adatok betöltése, előfeldolgozása, tokenizáció}

Az előző fejezetben ismertetett paraméterek beállítása után elkezdhetjük az adatok betöltését. Egyfelől az előre betanított \texttt{huBERT} neurális hálót, másfelől a tanításhoz szükséges \texttt{HuSST} címkézett adatokat.

\begin{enumerate}
    \item Adathalmaz betöltése a Hugging Face \texttt{datasets} könyvtárával
    \item Szövegek tokenizálása a huBERT tokenizálóval
    \item PyTorch DataLoader-ek létrehozása a tanításhoz
\end{enumerate}

A \texttt{HuSST} tanító, validációs és teszt adathalmazból áll a korábban ismertetett felépítéssel: egy magyar kijelentéshez vagy negatív, vagy semleges, vagy pozitív címke tartozik. Szemléltetésképp egy részlet a tanítási adathalmazból:

\begin{lstlisting}[language=json, caption=Minta a HuSST adathalmazból]
[
    {
        "text": "Azonban hiányzik belőle az a nagyság és hősiesség, ami Stevensont és a korábbi Disney-meséket jellemzi.",
        "label": "negative"
    },
    {
        "text": "Informatív, ha sok beszédes részt tartalmaz egy dokumentumfilm.",
        "label": "neutral"
    },
    {
        "text": "Ha szeretsz időnként moziba menni, érdemes a Wasabi-val kezdeni.",
        "label": "positive"
    }
]
\end{lstlisting}

\subsubsection{Tokenizáció}

A \texttt{HuSST} tokenizációját a \texttt{HuBERT} előre tanított tokenizálója végzi el. Ennek segítségével helyesen lesznek tagolva  a szavak a tanításhoz használt szöveg betöltésekor.

\begin{lstlisting}[language=Python,caption=Tokenizáció]
# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Create datasets
train_dataset = HungarianSentimentDataset(
    train_texts, train_labels, tokenizer, MAX_LENGTH
)
# Create dataloaders
train_loader = DataLoader(
    train_dataset, batch_size=BATCH_SIZE, shuffle=True
)
\end{lstlisting}

\subsection{3. lépés: Tanítás, elkészült modell mentése}

A folyamat végső lépéseként elkezdhető az új modell betanítása a korábban bemutatottak segítségével.

A \textit{train epoch} függvény felelős a modell egy epoch-on keresztüli tanításáért. A függvény először a modellt tanítási módba állítja, majd inicializálja a veszteség és az előrejelzések nyilvántartását. A tanítási ciklus a megadott adatokon halad végig, ahol minden kötegelt adatra három fő lépést hajt végre: az adatok mozgatása a megfelelő eszközre \textit{(CPU/GPU, jelenleg csak CPU áll rendelkezésre a tanításhoz)}, a forward és backward propagáció végrehajtása, valamint a paraméterek frissítése az optimizer segítségével. A veszteségfüggvény \textit{CrossEntropyLoss} értékelésével és a gradiensek visszaszámításával a modell súlyait finomhangolja.

\begin{lstlisting}[language=Python,caption=Tanítási folyamat]
# 3. Training functions
def train_epoch(model, data_loader, optimizer, device):
    model.train()
    total_loss = 0
    correct_predictions = 0

    for batch in tqdm(data_loader, desc="Training"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = nn.CrossEntropyLoss()(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, preds = torch.max(outputs, dim=1)
        correct_predictions += torch.sum(preds == labels)

    accuracy = correct_predictions.double() / len(data_loader.dataset)
    avg_loss = total_loss / len(data_loader)

    return avg_loss, accuracy
\end{lstlisting}

A folyamat utolsó szakaszában a modell tényleges betanítása történik meg a korábban definiált komponensek felhasználásával. A tanítási folyamat során a modell többször feldolgozza a teljes tanító adathalmazt, miközben a veszteségfüggvény minimalizálására törekszik. Minden futás után értékelni lehet a modell teljesítményét a validációs halmazon, ami lehetővé teszi a túlilleszkedés \textit{(overfitting)} felismerését. Az elkészült modellt a diszlre kerül mentésre, hogy predikciókat lehessen vele végezni a későbbiekben. A mentés során nem csak a modell súlyait, hanem a tokenizálót és a konfigurációs paramétereket is érdemes elmenteni.

\subsection{A modell tanítása közben kletkezett kimenet}

\begin{lstlisting}[style=docker]
C:\Temp\py\hu-sent\venv\Scripts\python.exe C:\Temp\py\hu-sent\learn.py

Epoch 1/3
------------------------------
Training: 100%|==========| 583/583 [2:23:42<00:00, 14.79s/it]
Train Loss: 0.6952, Accuracy: 0.6944
Evaluation: 100%|==========| 73/73 [05:27<00:00,  4.49s/it]
Val Loss: 0.5807, Accuracy: 0.7399
Saved new best model

Classification Report:
Training:   0%|          | 0/583 [00:00<?, ?it/s]              precision    recall  f1-score   support

    negative       0.75      0.91      0.82       697
     neutral       0.76      0.48      0.59       435
    positive       0.40      0.55      0.46        33

    accuracy                           0.74      1165
   macro avg       0.64      0.65      0.62      1165
weighted avg       0.75      0.74      0.73      1165


Epoch 2/3
------------------------------
Training: 100%|==========| 583/583 [2:14:58<00:00, 13.89s/it]
Train Loss: 0.4737, Accuracy: 0.8001
Evaluation: 100%|%|==========| 73/73 [05:00<00:00,  4.12s/it]
Val Loss: 0.6953, Accuracy: 0.6884

Classification Report:
              precision    recall  f1-score   support

    negative       0.79      0.80      0.80       697
     neutral       0.68      0.50      0.58       435
    positive       0.18      0.82      0.30        33

    accuracy                           0.69      1165
   macro avg       0.55      0.71      0.56      1165
weighted avg       0.74      0.69      0.70      1165


Epoch 3/3
------------------------------
Training: 100%|==========| 583/583 [2:11:09<00:00, 13.50s/it]
Train Loss: 0.3142, Accuracy: 0.8765
Evaluation: 100%|==========| 73/73 [04:58<00:00,  4.08s/it]
Val Loss: 0.7588, Accuracy: 0.7425
Saved new best model

Classification Report:
              precision    recall  f1-score   support

    negative       0.80      0.86      0.83       697
     neutral       0.74      0.56      0.64       435
    positive       0.27      0.70      0.39        33

    accuracy                           0.74      1165
   macro avg       0.60      0.71      0.62      1165
weighted avg       0.76      0.74      0.74      1165


Training complete. Best validation accuracy: 0.7425

Process finished with exit code 0
\end{lstlisting}

\section{Az elkészült modell felhasználása}

A betanított, elkészült modellt egy egyszerű \textit{Python} fájlban is tudjuk használni, vagy \textit{Jupyter Notebookban}. Ezek a módszerek alapvetően jók tesztelésre vagy további fejlesztésekhez, de nem túl felhasználóbarátok.

A könnyű használhatóság érdekében fontosnak tartottam valamilyen felhasználói grafikus interfész implementálását, ehhez a legegyszerűbben elkészíthető, multiplatform megoldást választottam: a webes felületet. A felület segítségével felhasználói oldalon bármilyen eszközről használható a rendszer, amelyen van böngésző. A webszervert és a modellt \textit{Docker} konténerben lehet futtatni \textit{docker compose} segítségével.

\subsection{Futtatás Python fájlból}

Első körben a kipróbálás és tesztelés legegyszerűbb módja, a predikcióhoz készült \textit{Python} fájl futtatása. Ennek egy példája látható a következő sorokban.

\begin{verbatim}
    (venv) C:\Temp\py\hu-sent>python prediction.py
    Text: Ez a film fantasztikus volt!
    Sentiment: positive
    
    Text: Nem tetszett a könyv.
    Sentiment: negative
    
    Text: Átlagos élmény volt, semmi különös.
    Sentiment: negative
    
    Text: Süt a nap.
    Sentiment: neutral
    
    Text: Esik az eső.
    Sentiment: neutral
    
    Text: Szép időnk van ma.
    Sentiment: positive
\end{verbatim}

\subsection{Futtatás webes felülettel}

A webes verzió futtatásához \textit{Docker compose} segítségével el kell indítani az alábbi szolgáltatásokat:

\begin{itemize}
    \item \textbf{Adatbázis szerver} (PostgreSQL): Felhasználói adatok és előzmények tárolása
    \item \textbf{API szerver} (Python Flask): A szentiment elemzés végrehajtása REST API-n keresztül
    \item \textbf{Webszerver} (Flask): Felhasználói felület megjelenítése
\end{itemize}

Futtatáskor a \textit{Docker} az alábbi kimenetet adja, amiből meggyőződhetünk róla, hogy minden szolgáltatás megfelelően el tudott indulni és elérhető. Ha mindez megtörtént, böngészővel tudunk csatlakozni a kiszolgáló IP címén, az 5000-es porton futó webszerverhez.

\begin{lstlisting}[style=docker]
(venv) C:\Temp\py\hu-sent\sentimentapp>docker compose up
time="2025-05-03T12:16:32+02:00" level=warning msg="C:\\Temp\\py\\hu-sent\\sentimentapp\\docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
[+] Running 3/3
  Container sentimentapp-db-1             Created                                                                                          0.0s
  Container sentimentapp-sentiment-api-1  Created                                                                                          0.0s
  Container sentimentapp-web-client-1     Created                                                                                          0.0s
Attaching to db-1, sentiment-api-1, web-client-1
db-1             |
db-1             | PostgreSQL Database directory appears to contain a database; Skipping initialization
db-1             |
db-1             | 2025-05-03 10:16:34.041 UTC [1] LOG:  starting PostgreSQL 13.20 (Debian 13.20-1.pgdg120+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
db-1             | 2025-05-03 10:16:34.041 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
db-1             | 2025-05-03 10:16:34.041 UTC [1] LOG:  listening on IPv6 address "::", port 5432
db-1             | 2025-05-03 10:16:34.043 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
db-1             | 2025-05-03 10:16:34.049 UTC [27] LOG:  database system was interrupted; last known up at 2025-05-02 18:35:34 UTC
db-1             | 2025-05-03 10:16:34.196 UTC [27] LOG:  database system was not properly shut down; automatic recovery in progress
db-1             | 2025-05-03 10:16:34.198 UTC [27] LOG:  redo starts at 0/160C470
db-1             | 2025-05-03 10:16:34.198 UTC [27] LOG:  invalid record length at 0/160C558: wanted 24, got 0
db-1             | 2025-05-03 10:16:34.198 UTC [27] LOG:  redo done at 0/160C520
db-1             | 2025-05-03 10:16:34.212 UTC [1] LOG:  database system is ready to accept connections
sentiment-api-1  |  * Serving Flask app 'app'
sentiment-api-1  |  * Debug mode: off
sentiment-api-1  | WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
sentiment-api-1  |  * Running on all addresses (0.0.0.0)
sentiment-api-1  |  * Running on http://127.0.0.1:5000
sentiment-api-1  |  * Running on http://172.18.0.3:5000
sentiment-api-1  | Press CTRL+C to quit
sentiment-api-1  | 127.0.0.1 - - [03/May/2025 10:17:09] "HEAD /health HTTP/1.1" 200 -
web-client-1     |  * Serving Flask app 'app'
web-client-1     |  * Debug mode: on
web-client-1     | WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
web-client-1     |  * Running on all addresses (0.0.0.0)
web-client-1     |  * Running on http://127.0.0.1:5000
web-client-1     |  * Running on http://172.18.0.4:5000
web-client-1     | Press CTRL+C to quit
web-client-1     |  * Restarting with stat
web-client-1     |  * Debugger is active!
web-client-1     |  * Debugger PIN: 118-356-955
sentiment-api-1  | 127.0.0.1 - - [03/May/2025 10:17:39] "HEAD /health HTTP/1.1" 200 -
\end{lstlisting}

\subsection{Webes szolgáltatások}

A rendszer rendelkezik felhasználókezeléssel, regisztrációs és bejelentkezési felülettel, valamint a felhasználók chatelőzményeinek tárolásával.

Néhány kép működés közben:

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/01_login.png}
    \caption{Bejelentkezési felület}
    \label{fig:01_login}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/02_register.png}
    \caption{Regisztrációs felület}
    \label{fig:02_register}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/03_chat.png}
    \caption{Üres chatfelület}
    \label{fig:03_chat}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/05_history.png}
    \caption{Előzmények törlése}
    \label{fig:05_history}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth,height=0.95\textheight,keepaspectratio]{images/04_chat.png}
    \caption{Chatfelület néhány példával}
    \label{fig:04_chat}
\end{figure}

\subsection{Adatbázis modell}

A rendszer két fő táblát használ:

\begin{table}[h]
\centering
\caption{Felhasználói tábla (\texttt{User})}
\begin{tabularx}{\linewidth}{|l|l|X|}
\hline
\textbf{Mező}           & \textbf{Típus} & \textbf{Leírás} \\ \hline
\texttt{id}             & Integer        & Elsődleges kulcs \\ \hline
\texttt{username}       & String(100)    & Egyedi felhasználónév \\ \hline
\texttt{password\_hash} & String(200)    & Titkosított jelszó \\ \hline
\texttt{created\_at}    & DateTime       & Regisztráció időpontja \\ \hline
\texttt{chats}          & Relationship   & A felhasználó chatüzenetei \\ \hline
\end{tabularx}
\label{tab:user}
\end{table}

\begin{table}[h]
\centering
\caption{Chat tábla (\texttt{Chat})}
\begin{tabularx}{\linewidth}{|l|l|X|}
\hline
\textbf{Mező}        & \textbf{Típus} & \textbf{Leírás} \\ \hline
\texttt{id}          & Integer        & Elsődleges kulcs \\ \hline
\texttt{user\_id}    & Integer        & Külső kulcs a \texttt{User} táblához \\ \hline
\texttt{message}     & String(1000)   & Felhasználó üzenete \\ \hline
\texttt{sentiment}   & String(20)     & Érzelmi értékelés eredménye \\ \hline
\texttt{confidence}  & Float          & A modell bizonyossága \\ \hline
\texttt{created\_at} & DateTime       & Üzenet időpontja \\ \hline
\end{tabularx}
\label{tab:chat}
\end{table}

\subsection{Adatbázis kapcsolatok}

\begin{itemize}
\item Egy felhasználó (\texttt{User}) több chatüzenettel (\texttt{Chat}) rendelkezhet
\item Minden chatüzenet pontosan egy felhasználóhoz tartozik
\end{itemize}

\subsection{Docker és Docker Compose}

Ahogy az korábban említve lett, a webes szentimentanalízis alkalmazás \textit{Docker} konténerizációval készült, a könnyű telepítés, hordozhatóság és reprodukálhatóság érdekében. A konténerizált környezet kiváló lehetőségeket nyújt a fejlesztés, tesztelés és üzemeltetés területén. A függőségek konténerekbe csomagolásával megszűnnek a kompatibilitási problémák és az alkalmazás bármely Docker-t támogató platformon futtatható. A szolgáltatások szükség szerint akár skálázhatók is. 

A szolgáltatások a Docker által biztosított belső hálózaton kommunikálnak egymással, a \textit{depends\_on} direktívák pedig biztosítják a megfelelő indítási sorrendet. Jelenleg:

\begin{itemize}
    \item Elsőként az adatbázis szerver
    \item Másodikként a szentiment API
    \item Végül a webszerver indul
\end{itemize}

A kötetek (\textit{volumes}) használata garantálja az adatmegőrzést a konténerek újraindítása esetén is. Perzisztenciára jelenleg csak az adatbázis esetében van szükség.

A \textit{Docker Compose} segítségével a három szükséges konténer kezelése egybe van szervezve, és a beállításaik tárolva vannak. Amennyiben a \textit{Docker} fájlok és a \textit{Docker Compose} fájl jól van elkészítve, a program indításához mindössze a \texttt{docker compose up --build} parancsot kell kiadni, és automatikusan lefutnak a folyamatok.

\subsection{Docker fájlok}

\begin{lstlisting}[
  style=docker,
  caption={docker-compose.yml}]
    version: '3.8'

    services:
      db:
        image: postgres:13
        environment:
          POSTGRES_USER: sentiment_user
          POSTGRES_PASSWORD: sentiment_pass
          POSTGRES_DB: sentiment_db
        volumes:
          - postgres_data:/var/lib/postgresql/data
        ports:
          - "5432:5432"
        healthcheck:
          test: ["CMD-SHELL", "pg_isready -U sentiment_user -d sentiment_db"]
          interval: 5s
          timeout: 5s
          retries: 5
    
      sentiment-api:
        build:
          context: .
          dockerfile: model/Dockerfile
        ports:
          - "5001:5000"
        environment:
          - MODEL_NAME=SZTAKI-HLT/hubert-base-cc
          - MAX_LENGTH=128
          - DATABASE_URL=postgresql://sentiment_user:sentiment_pass@db:5432/sentiment_db
        depends_on:
          db:
            condition: service_healthy
        healthcheck:
          test: ["CMD-SHELL", "wget -q --spider http://127.0.0.1:5000/health || exit 1"]
          interval: 30s
          timeout: 10s
          retries: 3
        restart: unless-stopped
    
      web-client:
        build:
          context: .
          dockerfile: client/Dockerfile
        ports:
          - "5000:5000"
        depends_on:
          sentiment-api:
            condition: service_healthy
          db:
            condition: service_healthy
        environment:
          - API_URL=http://sentiment-api:5000/predict
          - DATABASE_URL=postgresql://sentiment_user:sentiment_pass@db:5432/sentiment_db
          - SECRET_KEY=your-secret-key-here
        restart: unless-stopped
    
    volumes:
      postgres_data:    
\end{lstlisting}

\begin{lstlisting}[
  style=docker,
  caption={Sentiment API Dcokerfile}]
    FROM python:3.9-slim
    
    WORKDIR /app
    
    # Install necessary system dependencies
    RUN apt-get update && apt-get install -y gcc python3-dev curl wget && rm -rf /var/lib/apt/lists/*
    
    # Copy the requirements.txt specific to the model service
    COPY model/requirements.txt /app/requirements.txt
    
    # Install dependencies
    RUN pip install --no-cache-dir -r /app/requirements.txt
    
    # Copy the rest of the model code to the container's /app directory
    COPY model /app
    
    # Set the command to run the application (ensure the app.py is at /app/app.py)
    CMD ["python", "/app/app.py"]
\end{lstlisting}

\begin{lstlisting}[
  style=docker,
  caption={Webserver Dockerfile}]
    FROM python:3.9-slim
    
    WORKDIR /app
    
    COPY client/requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt
    
    COPY client/ /app
    
    RUN mkdir -p instance
    
    CMD ["python", "app.py"]
\end{lstlisting}

\section{Források}

A dokumentumot az alább felsorolt források segítségével készítettem el.

\begin{thebibliography}{9}

\bibitem{hubert-hf}
SZTAKI-HLT. \textit{hubert-base-cc}. Hugging Face. \\
\url{https://huggingface.co/SZTAKI-HLT/hubert-base-cc}

\bibitem{husset}
NYTK. \textit{HuSST Dataset}. Hugging Face. \\
\url{https://huggingface.co/datasets/NYTK/HuSST}

\bibitem{hubert-official}
SZTAKI-HLT. \textit{huBERT - Hungarian BERT Model}. BME-HLT. \\
\url{https://hlt.bme.hu/hu/resources/hubert}

\bibitem{awesome-nlp}
Orosz György. \textit{Awesome Hungarian NLP Resources}. GitBook. \\
\url{https://oroszgy.gitbook.io/awesome-hungarian-nlp-resources}

\bibitem{awesome-github}
Orosz György. (2023). \textit{Awesome Hungarian NLP}. GitHub. \\
\url{https://github.com/oroszgy/awesome-hungarian-nlp}

\bibitem{hubert-paper}
Laki László J., Yang Zijian Győző. (2022). \textit{huBERT - Hungarian BERT}. Acta Universitatis Óbuda. \\
\url{https://acta.uni-obuda.hu/Laki_Yang_134.pdf}

\bibitem{docker-debian}
Docker Inc. \textit{Install Docker Engine on Debian}. Docker Documentation. \\
\url{https://docs.docker.com/engine/install/debian/}

\bibitem{docker-postinstall}
Docker Inc. \textit{Linux post-installation steps for Docker Engine}. Docker Documentation. \\
\url{https://docs.docker.com/engine/install/linux-postinstall/}

\bibitem{bert-wiki}
Wikipedia contributors. \textit{BERT (language model)}. Wikipedia. \\
\url{https://en.wikipedia.org/wiki/BERT_(language_model)}

\bibitem{bert-guide}
Shaikh, Rayyan. \textit{A Comprehensive Guide to Understanding BERT: From Beginners to Advanced}. Medium. \\
\url{https://medium.com/@shaikhrayyan123/a-comprehensive-guide-to-understanding-bert-from-beginners-to-advanced-2379699e2b51}

\bibitem{sentiment-hungarian}
NYTK. \textit{sentiment-hts5-hubert-hungarian}. Hugging Face. \\
\url{https://huggingface.co/NYTK/sentiment-hts5-hubert-hungarian}

\end{thebibliography}


\end{document}

